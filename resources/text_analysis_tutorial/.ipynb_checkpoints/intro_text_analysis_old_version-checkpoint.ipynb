{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Basics of Computational Text Analysis (with Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- Introduction\n",
    "- Loading Text, Encodings, Memory\n",
    "    - Example\n",
    "    - Ressources\n",
    "- Manipulating Text: Regular Expressions\n",
    "    - Example\n",
    "    - Ressources\n",
    "- Tokenization\n",
    "    - Example\n",
    "    - Resources\n",
    "- Document Term Matrices\n",
    "    - Example\n",
    "    - Resources\n",
    "- The Sparsity Problem\n",
    "- Text Preprocessing\n",
    "- Stemming and Lemmatization\n",
    "- Additional Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "- Scope: Lay out what can be done. Concentrate on Bag-of-Words stuff. Explain basics how to get to tdm\n",
    "\n",
    "- What can and can't be done with computers\n",
    "\n",
    "- Common (analytic) tasks\n",
    "    - Supervised problems\n",
    "        - Text classification\n",
    "        - Sentiment Analysis\n",
    "        - Named Entity Recognition\n",
    "    - Unsupervised Problems\n",
    "        - Thematic Clustering (e.g. Topic Modelling)\n",
    "        - Information Retrieval\n",
    "        - Document similarity\n",
    "       \n",
    "- Text as Data\n",
    "    - Bag of Words\n",
    "    - Natural Language Tools\n",
    "    \n",
    "- From dirty text to a Document-Term-Matrix (DTM)\n",
    "DTM is a good basis for a lot of analyses (from counting words to sophisticated statistcal analysis). Is not always necessary but good to have. Most issues in natural language processing come up when creating / working with DTMs\n",
    "\n",
    "- What we will do:\n",
    "    1. Load text into python (Republican Debate)\n",
    "    2. Clean the text\n",
    "    3. Parse it into documents\n",
    "    4. Tokenize\n",
    "    5. Create Term Document Matrix\n",
    "    6. Reduce dimensionality\n",
    "    \n",
    "    \n",
    "I want to show basic concepts. \n",
    "I will program them from scratch, in practice you would use optimized packages. I don't go into detail here but will point to them in the in each section in the `Resources` subsection.\n",
    "\n",
    "Terminology:\n",
    "- Corpus\n",
    "- Document\n",
    "- Token\n",
    "- Term\n",
    "- Type\n",
    "- Document Term Matrix\n",
    "- Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Text, Encodings, Memory\n",
    "\n",
    "Write something here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "## Loading Text into memory\n",
    "raw_text_connection = io.open('rep_debate.txt', mode='r', encoding='utf-8')\n",
    "\n",
    "print raw_text_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_text = raw_text_connection.read()\n",
    "raw_text[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some explanations. The `u` in front of the output means this is a unicode string. Unicode is a character encoding that includes all possible characters. \\ stands for escaped characters. For example `\\n` is a new line `\\u201911` is the unicode code for `'`. To see the nicer formatting where special characters are interpreted we can `print` it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print raw_text[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_text_connection = io.open('rep_debate.txt', mode='r', encoding='ascii')\n",
    "raw_text = raw_text_connection.read()\n",
    "raw_text[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Working on streams\n",
    "raw_text_connection = io.open('rep_debate.txt', mode='r', encoding='utf-8')\n",
    "lengths = []\n",
    "\n",
    "for line_number, line in enumerate(raw_text_connection):\n",
    "    print line\n",
    "    lengths.append(len(line))\n",
    "    if line_number == 5:\n",
    "        break\n",
    "        \n",
    "print lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "- Intro to encodings with emphasis on unicode: https://www.w3.org/International/articles/definitions-characters/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating Text: Regular Expressions\n",
    "\n",
    "Regular expressions are basically templates that can be used to match character sequences. You have probably used them. For example when you type a search into a search engine you can use the `*` to match several terms with one query. For example `read*` would match `reading`, `reader`, `readings`, `reads`, etc. In this case `*` is a regular expression that says 'match every character except a space'. \n",
    "\n",
    "Regular expressions differe slightly between programming languages. This [regex tester](https://regex101.com/#python) is a really cool ressource to check out your regular expressions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the re (for regular expression) module\n",
    "import re\n",
    "\n",
    "# Some basics\n",
    "s = 'Good evening, Iâ€™m Carl Quintanilla, with my colleagues Becky Quick and John Harwood.'\n",
    "\n",
    "# Search stuff\n",
    "results = re.findall(pattern=r'[A-Z][a-z]+', string=s) # Find all Upper Case Words\n",
    "for result in results:\n",
    "    print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace stuff\n",
    "\n",
    "## Replace every `'`, ',', '.' with `!!!!!`\n",
    "print re.sub(pattern=r'[\\',\\.]', repl='!!!!!', string=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some basic expressions\n",
    "\n",
    "- `[]`: A set of characters/expressions. E.g. `[A-Z]` matches all upper case letters\n",
    "- `.`: Matches everything\n",
    "- `+`: Matches one or more of the preceding character\n",
    "- `?`: Matches 0 or one repetitions of the preceding character\n",
    "- `\\n`: Newline\n",
    "- `\\t`: Tab\n",
    "- `\\s`: Every space character, e.g. `\\n`, `\\t`, `\\r`\n",
    "\n",
    "Just some examples. See the [documentation]() for an exhaustive list and use Google and [regex tester](https://regex101.com/#python) extensively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Example\n",
    "\n",
    "Let's use it with our example. First we want to clean transcript from annotations like `(APPLAUSE)` or `(LAUGHTER)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print raw_text[1:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "annotation = re.compile(r'\\([A-Z]+\\)\\n?')\n",
    "annotation.findall(raw_text)[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_text = annotation.sub(repl='', string=raw_text)\n",
    "print new_text[1:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to make a separate text document for each utterance so we can treat them as separate documents. We observe that each speaker change is indicated by the following pattern: `[NewLine] + NAME + : + [NewLine]`. We can use this regularity to split the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speaker = re.compile(r'([A-Z]+:)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speaker.findall(string=new_text)[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works. Now we use this to make one big document per speaker containing all utterances by this speaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs = speaker.split(string=new_text)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(docs[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Empty dictionary to store the information. Format will be 'name': 'string of all their text'\n",
    "utterances = {}\n",
    "\n",
    "# Loop through the documents we created in the previous step\n",
    "for element in docs:\n",
    "    if element == '':\n",
    "        continue\n",
    "    if speaker.match(element):      \n",
    "        \n",
    "        current_speaker = re.sub(':', '', element)\n",
    "        \n",
    "        if current_speaker not in utterances.keys():\n",
    "            utterances[current_speaker] = ''\n",
    "    else:\n",
    "        utterances[current_speaker] = utterances[current_speaker] + ' ' + element\n",
    "\n",
    "# See for whom we got utterances\n",
    "print utterances.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Most statistical text analysis is based on the 'bag-of-words' approach: It is assumed (clearly wrongly) information in documents is purely contained in the word counts of a document. Grammatical and syntactical structure is ignored. Let's first count all words in each document.\n",
    "To do this we have to split the document into discrete words, they can are also called 'tokens' and the process 'tokenization'. Here we simply split the string object, whenever there is a white space (you might already think of things that can go wrong here). There are more sophisticated methods to do this but for now it is sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Let's see how this works for Donald Trumps utterances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Trumps utterances from our collection\n",
    "text = utterances['TRUMP']\n",
    "\n",
    "# Split the text on white space\n",
    "text = text.split(' ')\n",
    "\n",
    "# Empty dictionary to store 'word: wordcount' pairs \n",
    "trump = {}\n",
    "\n",
    "# Loop through each word in the text and count them\n",
    "for token in text:\n",
    "    if token not in  trump.keys():\n",
    "        trump[token] = 1\n",
    "    else:\n",
    "        trump[token] += 1\n",
    "\n",
    "# Print a sample of the word counts to check what we got\n",
    "for i, word in enumerate(trump):\n",
    "    print word + ': ' + str(trump[word])\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore that this looks weird. We will address the punctuation and lower/upper case issues in a bit. Let's first finish our first document-term-matrix.\n",
    "First we count the words for each speaker, as we did with Trump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Empty list to store each speaker's dictionary\n",
    "word_counts = []\n",
    "\n",
    "# Count all the words!\n",
    "for person in utterances:\n",
    "    text = utterances[person]\n",
    "    text = text.split(' ')\n",
    "    counts = {}\n",
    "    for token in text:\n",
    "        if token not in  counts.keys():\n",
    "            counts[token] = 1\n",
    "        else:\n",
    "            counts[token] += 1\n",
    "    word_counts.append(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- stanford nltk (look up the method)\n",
    "- spacy (fastest!)\n",
    "- R tm package\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Term Matrix\n",
    "\n",
    "A lot (but not all) statistical analyses of text are based on Document Term Matrices (DTM). A DTM, as the name says, is a matrix that contains one row per document (in our case a document is all the text for one candidate) and one column per term (or token). That means that each document is represented as a vector in a space that has as many dimensions as there are unique terms in the collection of all documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Collect all terms form all speakers\n",
    "vocabulary = set()\n",
    "for person in word_counts:\n",
    "    vocabulary.update(person.keys())\n",
    "print len(vocabulary)\n",
    "pprint(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dtm = pd.DataFrame(columns = list(vocabulary))\n",
    "i = 0\n",
    "for person in word_counts:\n",
    "    \n",
    "    vec = []\n",
    "    for word in dtm.columns:\n",
    "        if word in person:\n",
    "            vec.append(person[word])\n",
    "        else:\n",
    "            vec.append(0)\n",
    "    \n",
    "    dtm.loc[i] = vec\n",
    "    i += 1\n",
    "dtm\n",
    "print dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Resources\n",
    "\n",
    "- Python: sklearn.vectorizer\n",
    "- R: tm, benoits package\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Sparsity Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "n_non_zero = dtm.astype(bool).sum().sum()\n",
    "tot = 20 * 4360\n",
    "1 - n_non_zero / tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculation above show's that almost 90% of our matrix are zeros. This causes problems in a variety of ways: \n",
    "\n",
    "The matrix becomes very large for only minimal increase in information. Memory and computational requirements increase dramatically.\n",
    "\n",
    "Words that mean the same thing are treated as completely independent dimensions. E.g.:\n",
    "    - Capitalization: 'And', 'and', 'AND'\n",
    "    - Punctuation: 'dog,', 'dog.', 'dog'\n",
    "    - Grammatical inflections: 'walking' vs. 'walked' vs. 'walk'\n",
    "    - Irregular grammatical forms: 'dive', 'dove'\n",
    "    - etc.\n",
    "\n",
    "Therefore, documents that could be considered similar can seem very distant for the computer. E.g. consider three documents:\n",
    "        1. 'The dog eats a cat'\n",
    "        2. 'the dogs eat many bananas'\n",
    "        3. 'I like coffee very much'\n",
    "\n",
    "The corresponding term document matrix would look like this:\n",
    "\n",
    "```\n",
    "    The dog eats a cat the dogs eat many bananas I like coffee very much\n",
    "1   1   1   1    1 1   0   0    0   0    0       0 0    0      0    0\n",
    "2   0   0   0    0 0   1   1    1   1    1       0 0    0      0    0\n",
    "3   0   0   0    0 0   0   0    0   0    0       1 1    1      1    1\n",
    "```\n",
    "\n",
    "Calculating the euclidian distance (or any other metric) between the vector for each word shows that they are all equidistant from each other. However, one could argue that documents `1` and `2` are closer to each other than they are to document `3`.\n",
    "\n",
    "Now you probably say 'of course, we have to make everything lowercase, remove all the punctuation and stem the tokens and remove all the stopwords'. These steps are often taken as a routine in text analysis applications. In most cases most of these standard pre processing steps make absolute sense, but I think they should not be done mindlessly. Depending on the analysis some of these things that we remove here might be of value later. For example, capitalization of words might be important to differentiate between proper names and other words. The use of punctuation might be very informative when trying to attribute text to a specific person (don't we all know someone who uses way too many exclamation marks!!!!).\n",
    "\n",
    "Instead, I will discuss these steps as feature selection or dimensionality reduction techniques. Although they are normally separate steps in the analysis and there are statistical techniques to do them, I think it makes conceptually sense to treat them as the same. Consider using some form of latent factor analysis to retrieve a certian number of factors from the DTM and represent the documents in this reduced space. Although technically different, it is conceptually similar to converting tokens to lower case: We would expect that `Dog` and `dog` are highly correlated in the data because they mean basically the same thing. Therefore, we can represent the two variables as one, that we label `dog` and load with the sum of the old variables.\n",
    "\n",
    "There are many ways to deal with sparsity, but almost all of them involve reducing the size of the vocabulary, while maintaining as much relevant information as possible. Here is a list of things we can do:\n",
    "    1. Convert all words to lowercase\n",
    "    2. Remove all punctuation, numbers, special characters, etc.\n",
    "    3. Remove very common words that contain little semantic information like: `and`, `or`, `the`, etc. also called *stopwords*\n",
    "    4. Remove very infrequent words. Words that appear only once in the whole corpus are likely typos. Very infrequent words are also not very relevant for statistical analysis (but see above, depending on the analysis and outlier can be very informative).\n",
    "    5. Stemming \n",
    "    6. Lemmatization\n",
    "    7. Use dimension reduction techniques such as PCA, Factor Analysis, Neural Networks, Topic Models, etc. to locate documents in a [semantic space](https://en.wikipedia.org/wiki/Word_embedding). \n",
    "    8. Reduce vocabulary by inspecting the information content the words have for a supervise task, for example with $$\\chi^2$$ -test\n",
    "    \n",
    "For this basic tutorial I will demonstrate just the basics (`1`-`7`) and completely ignore the statistical techniques. I hope to do a more advanced tutorial where I will cover these topics.\n",
    "\n",
    "Now lets do this. `1`, `2` and `3` can be easily done with regular expressions and standard string tools in every programming language. We will do this on the utterances we split (since we need the colon, parentheses and upper case words for our text cleaning (see above).\n",
    "\n",
    "First we remove all everything that is not a normal letter or a space (or a colon, since we need it to identify the speakers, we will remove it later) with a regular expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First generate two regular expressions\n",
    "non_alpha = re.compile(r'[^A-Za-z ]') # Everything that is not a letter or a space\n",
    "excess_space = re.compile(r' +') # One or more spaces\n",
    "\n",
    "# Load a stopword list\n",
    "stopwords = set(io.open('stopwords.txt', 'r').read().split('\\n'))\n",
    "\n",
    "utterances_clean = {}\n",
    "\n",
    "for person in utterances:\n",
    "        \n",
    "    text = utterances[person]\n",
    "    # convert everything to lowercase\n",
    "    text = text.lower()\n",
    "      \n",
    "    # Remove stopwords\n",
    "    text = [w for w in text.split(' ') if w not in stopwords]\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    # removce non-letters and excess space\n",
    "    text = non_alpha.sub(repl=' ', string=text)\n",
    "    text = excess_space.sub(repl=' ', string=text)\n",
    "    \n",
    "    utterances_clean[person] = text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some word clouds to see how well it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "wordcloud = WordCloud().generate(utterances_clean['TRUMP'])\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "print 'Trump'\n",
    "plt.show()\n",
    "wordcloud = WordCloud().generate(utterances_clean['CRUZ'])\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "print 'Cruz'\n",
    "plt.show()\n",
    "wordcloud = WordCloud().generate(utterances_clean['BUSH'])\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "print 'Bush'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's regenerate the DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_counts = []\n",
    "\n",
    "# Count all the words!\n",
    "for person in utterances_clean:\n",
    "    text = utterances_clean[person]\n",
    "    text = text.split(' ')\n",
    "    counts = {}\n",
    "    for token in text:\n",
    "        if token not in  counts.keys():\n",
    "            counts[token] = 1\n",
    "        else:\n",
    "            counts[token] += 1\n",
    "    word_counts.append(counts)\n",
    "    \n",
    "# Collect all terms form all speakers\n",
    "vocabulary = set()\n",
    "for person in word_counts:\n",
    "    vocabulary.update(person.keys())\n",
    "    \n",
    "# Generate DTM\n",
    "dtm = pd.DataFrame(columns = list(vocabulary))\n",
    "i = 0\n",
    "for person in word_counts:\n",
    "    \n",
    "    vec = []\n",
    "    for word in dtm.columns:\n",
    "        if word in person:\n",
    "            vec.append(person[word])\n",
    "        else:\n",
    "            vec.append(0)\n",
    "    \n",
    "    dtm.loc[i] = vec\n",
    "    i += 1\n",
    " \n",
    "print dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "Stemming and lemmatization are two techniques to 'normalize' tokens. This is done to avoid differentiating between different grammatical forms of the same word. Consider the three examples: \n",
    "    - (walk, walking) \n",
    "    - (dive, dove) \n",
    "    - (doves, dove)\n",
    "    - (is, are)\n",
    "    \n",
    "The first is simple, the second is an irregular verb and the third is an animal. \n",
    "\n",
    "### Stemming\n",
    "\n",
    "Stemming algorithms are rule based and operate on the tokens itself. It returns the 'stem' of a word, i.e. without grammatical inflections etc. For the above example it would probably return something like:\n",
    "    - (walk, walk)\n",
    "    - (div, dov)\n",
    "    - (dov, dov)\n",
    "    - (is, are)\n",
    "It worked fine in the first place, but stemmers are not able to find the cannonical form (or lemma) of a word. Therefore it failed to figure out the last three cases.\n",
    "\n",
    "### Lemmatization\n",
    "\n",
    "Lemmatization, as the name suggests, is a group of algorithms that allow to find the lemma of a word. It often depends  on the context what the real lemma is (for example `the dove flies` or `I dove into the data`). Therefore, lemmatization works better if the algorithm is applied to the text in it's original form. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from spacy.en import English\n",
    "import Stemmer # PyStemmer module\n",
    "\n",
    "stemmer = Stemmer.Stemmer('english')\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = docs[16]\n",
    "text = re.sub('\\n', ' ', text)\n",
    "tokens = text.split(' ')\n",
    "\n",
    "# Lets first try stemming\n",
    "for i, token in enumerate(tokens):\n",
    "    print '{} -> {}'.format(token, stemmer.stemWord(token))\n",
    "    if i == 17:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now lemmatizing\n",
    "parsed_text = parser(text)\n",
    "\n",
    "for i, token in enumerate(parsed_text):\n",
    "    print '{} -> {}'.format(token.orth_, token.lemma_)\n",
    "    if i == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make our final DTM. We redo all the steps above and add the lemmatization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First generate two regular expressions\n",
    "non_alpha = re.compile(r'[^A-Za-z ]') # Everything that is not a letter or a space\n",
    "excess_space = re.compile(r' +') # One or more spaces\n",
    "\n",
    "# Load a stopword list\n",
    "stopwords = set(io.open('stopwords.txt', 'r').read().split('\\n'))\n",
    "\n",
    "utterances_clean = {}\n",
    "\n",
    "for person in utterances:\n",
    "        \n",
    "    text = utterances[person]\n",
    "    \n",
    "    parsed_text = parser(text)\n",
    "    lemmas = [token.lemma_ for token in parsed_text]\n",
    "    text = ' '.join(lemmas)\n",
    "    \n",
    "    # convert everything to lowercase\n",
    "    text = text.lower()\n",
    "      \n",
    "    # Remove stopwords\n",
    "    text = [w for w in text.split(' ') if w not in stopwords]\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    # removce non-letters and excess space\n",
    "    text = non_alpha.sub(repl=' ', string=text)\n",
    "    text = excess_space.sub(repl=' ', string=text)\n",
    "    \n",
    "    utterances_clean[person] = text\n",
    "\n",
    "word_counts = []\n",
    "\n",
    "# Count all the words!\n",
    "for person in utterances_clean:\n",
    "    text = utterances_clean[person]\n",
    "    text = text.split(' ')\n",
    "    counts = {}\n",
    "    for token in text:\n",
    "        if token not in  counts.keys():\n",
    "            counts[token] = 1\n",
    "        else:\n",
    "            counts[token] += 1\n",
    "    word_counts.append(counts)\n",
    "    \n",
    "# Collect all terms form all speakers\n",
    "vocabulary = set()\n",
    "for person in word_counts:\n",
    "    vocabulary.update(person.keys())\n",
    "    \n",
    "# Generate DTM\n",
    "dtm = pd.DataFrame(columns = list(vocabulary))\n",
    "i = 0\n",
    "for person in word_counts:\n",
    "    \n",
    "    vec = []\n",
    "    for word in dtm.columns:\n",
    "        if word in person:\n",
    "            vec.append(person[word])\n",
    "        else:\n",
    "            vec.append(0)\n",
    "    \n",
    "    dtm.loc[i] = vec\n",
    "    i += 1\n",
    " \n",
    "print dtm.shape\n",
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this matrix can be used for a variety of analyses. Ranging from simple word counts to sophisticated statistical models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Resources\n",
    "\n",
    "\n",
    "### Books / Articles / Videos\n",
    "\n",
    "\n",
    "### Software\n",
    "\n",
    "\n",
    "#### Stanford Nltk\n",
    "\n",
    "\n",
    "#### Python\n",
    "\n",
    "\n",
    "#### R\n",
    "\n",
    "\n",
    "#### Commandline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
